# -*- coding: utf-8 -*-
"""Transformers - CIS 519 Final Project - Amy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12ZP48uCtOzqyouGdjXFyThd6XRtKBeL0
"""

import numpy as np 
import pandas as pd
import json
import matplotlib
import matplotlib.pyplot as plt
from datetime import datetime
import glob
import seaborn as sns
import re

import torch
import torchvision

import torch.utils.tensorboard as tb

from PIL import Image

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

import random 
import os, math

!pip install simpletransformers==0.60.9

!pip install torch torchvision

import shutil
import torch
import torchvision
import torch.nn as nn
from torchsummary import summary
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

df = pd.read_csv('/content/US_youtube_trending_data.csv')

#df = df[0:20000]
#df.columns

#drop all values with 0 or any nans
df = df.dropna(how='any')
df.reset_index(drop=True, inplace=True)
remove_zero_likes = df[df['likes'] == 0 ].index
df.drop(remove_zero_likes, inplace=True)
remove_zero_dislikes = df[df['dislikes'] == 0 ].index
df.drop(remove_zero_dislikes, inplace=True)
remove_zero_views = df[df['view_count'] == 0 ].index
df.drop(remove_zero_views, inplace=True)
remove_zero_comment = df[df['comment_count'] == 0 ].index
df.drop(remove_zero_comment, inplace=True)

#drop all duplicates trending videos
df.sort_values('view_count', inplace=True)
df.drop_duplicates('title', keep='last', inplace=True)
df.reset_index(drop=True, inplace=True)

X = df.drop(columns=['super_trending'])
y = df['super_trending']

#calculate engagement rate based on interactions and highest # of views
#rate < 4% is considered mediocre interest or worst
def calculate_sentiment():
  a = []
  b = []
  for x in range(0, len(df['view_count'])):
    if (pd.notna(df['likes'][x]) and pd.notna(df['dislikes'][x]) and pd.notna(df['likes'][x])): 
      like = df['likes'][x]
      dislike = df['dislikes'][x]
      view = df['view_count'][x]
      comments = df['comment_count'][x]

      ratio = like / (like + dislike)
      engagement =(like + dislike + comments) / view   
      a.append(ratio)
      b.append(engagement)    
  df['ratio'] = a
  df['engagement'] = b

calculate_sentiment()

def replace_labels():
  df['engagement_label'] = df['engagement'].apply(lambda x: 1 if x > .04 else 0)

replace_labels()

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV

#data analysis pt1
f = list(df.groupby('categoryId'))
z = [len(group[1]) for group in f]
id = [str ((group[0])) for group in f]
names = ['Film & Animation', 'Auto & Vehicles', 'Music', 'Pets', 'Sports',
         'Travel & Events', 'Gaming', 'People & Blogs', 'Comedy', 'Entertainment',
         'News & Politics', 'Howto & Style', 'Education', 'Science & Tech', 'Nonprofit']
x_labels = [(str (id[l]) + ' - ' + names[l]) for l in range(0, len(names)) ]

#number of videos for each id
plt.title('category and # of videos in each')
plt.xlabel('category Ids')
plt.ylabel('# of trending videos')
plt.xticks([r for r in range(0, len(f))], x_labels, rotation=90)
plt.bar(x_labels, z, color='lightblue', width=0.4 )
plt.show()

#avg engagement level
plt.title('average engagement levels')
plt.xlabel('category Ids')
plt.ylabel('average engagement level')
y_avg = df.groupby('categoryId')['engagement'].mean()
plt.xticks([r for r in range(0, len(f))], x_labels, rotation=90)
df.reset_index()


plt.plot(x_labels,y_avg, 'bo')
plt.grid()
plt.show()

#importing countvectors/tfid
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer

import re
import string

!pip install emoji --upgrade
import emoji
!pip install num2words
from num2words import num2words

#using afinn lexicon for unsupervised lexicon learning
!pip install afinn
from afinn import Afinn
!pip install --user -U nltk 
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import PorterStemmer
from nltk.tag.mapping import tagset_mapping
from nltk import pos_tag
from nltk.stem.wordnet import WordNetLemmatizer
import nltk.data

nltk.download('words')
nltk.download('tagsets')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('universal_tagset')
nltk.download('names')
nltk.download('wordnet')

#reading the sentimenal lexicon (AFINN)
scoring = pd.read_csv('AFINN-en-165.txt', header = None)
scoring = scoring[0].str.split('\t', n = 1 , expand=True)
scoring.columns = ['word', 'score']
scoring.to_csv('AFINN-en-165.csv', index = None)
scoring = pd.read_csv('AFINN-en-165.csv')


nltk.help.upenn_tagset()
nltk.download('averaged_perceptron_tagger')
afinn = Afinn(emoticons=True)

afn = Afinn()
pst = PorterStemmer()
stoplist = nltk.corpus.words.words('en')
common_names = nltk.corpus.names.words()

#preprocessing the data
#https://www.nltk.org/_modules/nltk/tokenize/destructive.html#MacIntyreContractions 

contractions = ['don', 'wha', 'that', 'you', 'let', 'she', 'he', 'it', 'can', 'they', 'where', 'who', 'shouldn', 'should']
def replace_contractions(sentence):
  s = re.findall(r"\w+[a-z]+'[a-z]+", sentence)
  hold = s
  unmatch = ''
  r = ''
  if (len(s) != 0):
      hold = str(s[0])
      unmatch = re.findall("\w+", str(s[0]))
      v = unmatch[0]
      s = unmatch[0].lower()
      if (s in nltk.corpus.stopwords.words('english') or s in contractions 
          and s not in nltk.corpus.stopwords.words('english')):
        if(unmatch[1] == 't'):
          if(s != 'can'):
            s = s.replace('n', '')
            if(s == 'wo' or s == 'won'):
              s = 'will'
            if(s == 'ai'):
              s = 'am'
          s = s + ' not'
        if(unmatch[1] == 're'):
          s = s + ' are'
        if(unmatch[1] == 's'):
          if (s != 'let'):
            s = s + ' is'
          else:
            s = s + ' us'
        if(unmatch[1] == 've'):
          s = s + ' have'
        if(unmatch[1] == 'll'):
          s = s + ' will'
        if(unmatch[1] == 'd'):
          s = s + ' had'
  if type(hold) is list:
    v = sentence
  else:
    v = sentence.replace(hold, s)
  return v


def clean(column, type):
  result = column.copy()
  print(result)
  result = result.apply(lambda x: (replace_contractions(x)))
  result = result.replace('\.+', '.', regex=True)
  result = result.str.replace('[^\w\s]','', regex=True)

  result = result.apply(lambda x: ' '.join(emoji.demojize(x) for x in x.split()))
  result = result.apply(lambda x: ' '.join(x.lower() for x in x.split()))
  result = result.apply(lambda x: ' '.join(num2words(x) if x.isdigit() else x for x in x.split()))

  result = result.apply(lambda x: ' '.join(x for x in x.split() if x not in nltk.corpus.stopwords.words('english')))
  result = result.apply(lambda x: ' '.join(x for x in x.split() if x not in common_names))
  df[type + '_tokenized'] = result
  return result

clean(df['title'], 'title')

#Lemmatization 

def wordnet_tags(tag):
  if (tag.startswith('V')):
    return wordnet.VERB
  elif (tag.startswith('J')):
    return wordnet.ADJ
  elif (tag.startswith('N')):
    return wordnet.NOUN
  elif (tag.startswith('R')):
    return wordnet.ADV
  else:
    return None

def clean_lemmatization(column, type):
  lem = WordNetLemmatizer()
  penn_treebank = tagset_mapping('en-ptb', 'universal')
  #df[type + '_tokenized_lem'] = column.apply(lambda x:[(x, wordnet_tags(tag)) for x, tag in pos_tag(word_tokenize(x))])
  df[type + '_tokenized_lem'] = column.apply(lambda x:[(x, wordnet_tags(tag)) for x, tag in pos_tag(word_tokenize(x)) if wordnet_tags(tag) != wordnet.NOUN])
  df[type + '_tokenized_lem'] = df[type + '_tokenized_lem'].apply(lambda x: [lem.lemmatize(z[0], z[1]) if z[1] is not None else z[0] for z in x ])
  return df[type + '_tokenized_lem']

clean_lemmatization(df['title_tokenized'], 'title')

#afinn implementation with word tokenization 
def afinn_labeling(type, clean): 
  df['afinn_sentences'] = df[type + '_tokenized_lem'].apply(lambda x: [(x, afinn.score(x))])
  dummy = df['afinn_sentences'].apply(lambda x: [word[1] for word in x])
  df['afinn_score'] = dummy.apply(lambda x: np.sum(x))
  df['afinn_sentences'] = df['afinn_sentences'].apply(lambda x: ' '.join(word for word, tag in x))

afinn_labeling('title', 'stem')
#X = df['afinn_sentences']
#y = df['afinn_score']

X = df['title_tokenized_lem'].apply(lambda x: ' '.join(word for word in x))
y = df['engagement_label']

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=.2, random_state=42)

#bag of words 
count_vectorizer = CountVectorizer()
count_vectorizer_train_feat = count_vectorizer.fit_transform(X_train)

count_analyze = count_vectorizer.build_analyzer()

count_vectorizer_test_feat = count_vectorizer.transform(X_test)

#term frequency - inverse document frequency
tfid_vectorizer = TfidfVectorizer()
tfid_vectorizer_train_feat= tfid_vectorizer.fit_transform(X_train)

tfid_analyze = tfid_vectorizer.build_analyzer()

tfid_vectorizer_test_feat= tfid_vectorizer.transform(X_test)

!pip install mglearn
import mglearn

from sklearn.linear_model import LogisticRegression
from sklearn import svm, metrics

logistic_regression = LogisticRegression(max_iter=150)

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(logistic_regression, param_grid, cv = 10)
cross_val = cross_val_score(logistic_regression,count_vectorizer_train_feat, y_train, scoring='accuracy', cv = 10 )
cross_val_td = cross_val_score(logistic_regression,tfid_vectorizer_train_feat, y_train, scoring='accuracy', cv = 10 )

_ = grid.fit(count_vectorizer_train_feat, y_train)

logistic_regression.fit(count_vectorizer_train_feat, y_train)
logistic_regression.predict(count_vectorizer_test_feat)
print(logistic_regression.score(count_vectorizer_test_feat, y_test))


#logistic_regression.fit(tfid_vectorizer_train_feat, y_train)
#logistic_regression.predict(tfid_vectorizer_test_feat)
#print(logistic_regression.score(tfid_vectorizer_test_feat, y_test))

feature_names = count_vectorizer.get_feature_names()

mglearn.tools.visualize_coefficients(grid.best_estimator_.coef_, feature_names, n_top_features=25)
plt.title('top 25 words based on pos/neg contribution w/o nouns')
plt.show()

df['engagement'].describe()
print()