# -*- coding: utf-8 -*-
"""Copy of CIS 519 Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/110BGCqy1xxbNC5oZnP__1IOEuKm91-hD

# Importing
"""

import numpy as np 
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from datetime import datetime

import seaborn as sns
import re

import torch
import torchvision

from PIL import Image

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

import random 
import os, math

df = pd.read_csv('/content/US_youtube_trending_data.csv')

"""# Cleaning Up the Data"""

df = df.head(4000)

df

df['trending_date'] = pd.to_datetime(df['trending_date'], format= "%Y-%m-%dT%H:%M:%SZ")
df['publishedAt'] = pd.to_datetime(df['publishedAt'], format= "%Y-%m-%dT%H:%M:%SZ")
new_format = "%Y-%m-%d"
df['trending_date'] = df['trending_date'].apply(lambda x: pd.to_datetime(x.strftime(new_format)))
df['publishedAt'] = df['publishedAt'].apply(lambda x: pd.to_datetime(x.strftime(new_format)))

df['time_elapsed'] = df['trending_date'] - df['publishedAt']
df['time_elapsed'] = df['time_elapsed'].dt.days
df = df[df.time_elapsed != 0]

df['views_per_day'] = df['view_count'] / df['time_elapsed']
df['views_per_day'] = df['views_per_day'].astype(int)
df['super_trending'] = df['views_per_day'] > 300000
df['super_trending'] = df['super_trending'].astype(int)

df['likes_per_day'] = df['likes'] / df['time_elapsed']
df['likes_per_day'] = df['likes_per_day'].astype(int)
df['dislikes_per_day'] = df['dislikes'] / df['time_elapsed']
df['dislikes_per_day'] = df['dislikes_per_day'].astype(int)
df['comment_count_per_day'] = df['comment_count'] / df['time_elapsed']
df['comment_count_per_day'] = df['comment_count_per_day'].astype(int)

df["tags_count"]=df['tags'].apply(lambda x: x.count('|')+1)

df['comments_disabled'] = df['comments_disabled'].apply(lambda x: 1 if x else 0)
df['ratings_disabled'] = df['ratings_disabled'].apply(lambda x : 1 if x else 0)

!pip install afinn

from afinn import Afinn
afinn = Afinn(language='en')

df['title_sent_score'] = df['title'].apply(afinn.score)

df

df_clean = df.drop(columns=['video_id', 'title', 'publishedAt', 'channelId', 'channelTitle', 'trending_date', 'tags', 'view_count',
'likes', 'dislikes', 'comment_count', 'thumbnail_link', 'description',  'time_elapsed', 'views_per_day'])

df_clean

df_clean.describe()

categories = pd.get_dummies(df['categoryId'], prefix = 'category')

one_hot_df = pd.concat([df_clean, categories], axis = 1)

one_hot_df = one_hot_df.drop(columns=['categoryId'])

"""# Classification with Numerical Features """

#define train and test set for numerical classification
X = one_hot_df.drop(columns=['super_trending'])
y = one_hot_df['super_trending']

#train test split for numerical classification
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)
transformed_X_train = scaler.transform(X_train)
transformed_X_test = scaler.transform(X_test)

lr = LogisticRegression()
lr_train_result = lr.fit(transformed_X_train,y_train)

from sklearn import metrics
metrics.plot_roc_curve(lr, transformed_X_test, y_test)  
plt.show()

lr_y_pred = lr.predict(transformed_X_test)

from sklearn.metrics import accuracy_score
accuracy_score(y_test, lr_y_pred)

#from sklearn.metrics import f1_score
#f1_score(y_test, lr_y_pred)

from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV

distributions = dict(C=uniform(loc=0, scale=4))

lr_2 = LogisticRegression()
lr_2_cv = RandomizedSearchCV(lr_2, distributions, cv = 5)

lr_cv_result = lr_2_cv.fit(transformed_X_train, y_train)
print("Tuned Logistic Regression Parameters: {}".format(lr_cv_result.best_params_))
print("Best score is: {}".format(lr_cv_result.best_score_))

#lr_tuned = LogisticRegression(C = 3.647)
lr_tuned = LogisticRegression(C = lr_cv_result.best_params_.get('C'))
lr_tuned.fit(transformed_X_train,y_train)

metrics.plot_roc_curve(lr_tuned, transformed_X_test, y_test)  
plt.show()

lr_y_pred_tuned = lr_tuned.predict(transformed_X_test)

accuracy_score(y_test, lr_y_pred_tuned)

#f1_score(y_test, lr_y_pred_tuned)

"""# Support Vector Machines"""

from sklearn.svm import SVC
svm = SVC()
svm.fit(transformed_X_train, y_train)

metrics.plot_roc_curve(svm, transformed_X_test, y_test)  
plt.show()

svm_y_pred = svm.predict(transformed_X_test)

accuracy_score(y_test, svm_y_pred)

#f1_score(y_test, svm_y_pred)

Cs = [0.001, 0.01, 0.1, 1, 10]
kernels =  ['linear', 'poly', 'rbf', 'sigmoid']
gammas = [0.001, 0.01, 0.1, 1]
param_grid = {'C': Cs, 'kernel' : kernels, 'gamma': gammas}
svm_2 = SVC()
svm_2_cv = RandomizedSearchCV(svm_2, param_grid, cv=5)
svm_cv_result= svm_2_cv.fit(transformed_X_train, y_train)
print("Tuned SVM Parameters: {}".format(svm_cv_result.best_params_))
print("Best score is: {}".format(svm_cv_result.best_score_))

#svm_tuned = SVC(C = 10, kernel='poly', gamma = 1)
svm_tuned = SVC(C = svm_cv_result.best_params_.get('C'), kernel=svm_cv_result.best_params_.get('kernel'), 
                gamma = svm_cv_result.best_params_.get('gamma'))
svm_tuned.fit(transformed_X_train,y_train)

metrics.plot_roc_curve(svm_tuned, transformed_X_test, y_test)  
plt.show()

svm_y_pred_tuned = svm_tuned.predict(transformed_X_test)

accuracy_score(y_test, svm_y_pred_tuned)

#f1_score(y_test, svm_y_pred_tuned)

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train,y_train)

metrics.plot_roc_curve(rf, X_test, y_test)  
plt.show()

rf_y_pred = rf.predict(X_test)

accuracy_score(y_test, rf_y_pred)

#f1_score(y_test, rf_y_pred)

n_estimators = [10, 100, 1000]
max_features = ['sqrt', 'log2']
param_grid = {'n_estimators': n_estimators, 'max_features': max_features}
rf_2 = RandomForestClassifier()
rf_2_cv = RandomizedSearchCV(rf_2, param_grid, cv=5, n_iter=6)
rf_cv_result = rf_2_cv.fit(X_train, y_train)
print("Tuned SVM Parameters: {}".format(rf_cv_result.best_params_))
print("Best score is: {}".format(rf_cv_result.best_score_))

#rf_tuned = RandomForestClassifier(n_estimators=100, max_features='sqrt')
rf_tuned = RandomForestClassifier(n_estimators=rf_cv_result.best_params_.get('n_estimators'), max_features=rf_cv_result.best_params_.get('max_features'))
rf_tuned.fit(X_train,y_train)

metrics.plot_roc_curve(rf_tuned, X_test, y_test)  
plt.show()

rf_y_pred_tuned = rf_tuned.predict(X_test)

accuracy_score(y_test, rf_y_pred_tuned)

#f1_score(y_test, rf_y_pred_tuned)

"""# Classification with Image Recognition"""

import requests
from io import BytesIO
def check_valid(url):
  response = requests.get(url)
  img = Image.open(BytesIO(response.content))
  if (img.format == 'JPEG'):
    return True
  return False

#Running this on the thumbnail links showed that all the entries in that column were valid, because it takes
#a long time to run this, I just commented it out.

#df_valid = df['thumbnail_link'].apply(lambda x: check_valid(x))

#df_valid

!pip install torch torchvision

import shutil
import torch
import torchvision
import torch.nn as nn
from torchsummary import summary
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#check the device used
print(torch.__version__)
print(device)

#define train and test set for image recognition
imageX = df['thumbnail_link']
imageY = df['super_trending']

#train test split for image reconigtion
imageX_train, imageX_test, imageY_train, imageY_test = train_test_split(imageX, imageY, test_size=0.2, random_state=42)

#make lists
img_train_links = imageX_train.tolist()
img_test_links = imageX_test.tolist()
img_train_labels = imageY_train.tolist()
img_test_labels = imageY_test.tolist()

class MyDataset(Dataset):
    def __init__(self, img_list, label_list, data_transforms=None):
        """
        Your code here
        Hint: Use the python csv library to parse labels.csv
        """
        super(MyDataset, self).__init__()
        self.img_list = img_list
        self.labels = label_list
        self.transform = data_transforms

    def __len__(self):
        """
        Return the sample size
        """
        return len(self.labels)

    def __getitem__(self, idx):
        """
        Your code here
        return a tuple: img, label
        """
        response = requests.get(self.img_list[idx])
        img = Image.open(BytesIO(response.content))
        label = self.labels[idx]
        img = np.asarray(img).transpose(-1, 0, 1)
        #img = torch.from_numpy(np.asarray(img))
        
        if self.transform is not None:
             img = self.transform(img)
        return img, label

train_dataset = MyDataset(img_train_links, img_train_labels)
test_dataset = MyDataset(img_test_links, img_test_labels)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)

import torch.nn.functional as F
class CNNClassifier(torch.nn.Module):
    def __init__(self):
        super(CNNClassifier, self).__init__() 
        '''
        self.cov1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.norm1 = nn.BatchNorm2d(32)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)

        self.cov2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)
        self.norm2 = nn.BatchNorm2d(64)
        self.relu2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(2)

        self.fc1 = nn.Linear(in_features=38976, out_features=120)
        self.drop = nn.Dropout2d(0.25)
        self.fc2 = nn.Linear(in_features=120, out_features=2)
        '''

        self.cov1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)
        self.norm1 = nn.BatchNorm2d(16)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)

        self.cov2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)
        self.norm2 = nn.BatchNorm2d(32)
        self.relu2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(2)

        self.cov3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)
        self.norm3 = nn.BatchNorm2d(64)
        self.relu3 = nn.ReLU()
        self.maxpool3 = nn.MaxPool2d(2)

        self.fc1 = nn.Linear(in_features=7488, out_features=120)
        self.drop = nn.Dropout2d(0.25)
        self.fc2 = nn.Linear(in_features=120, out_features=2)
        

    def forward(self, x): 
        batch_size = x.size(0)
        x = x.float()
        x=self.cov1(x)
        x=self.norm1(x)
        x=self.relu1(x)
        x=self.maxpool1(x)
        
        x=self.cov2(x)
        x = self.norm2(x)
        x=self.relu2(x)
        x=self.maxpool2(x)
        #print(x.size())

        x=self.cov3(x)
        x = self.norm3(x)
        x=self.relu3(x)
        x=self.maxpool3(x)
        #print(x.size())

        x = x.view(batch_size, -1)
        x = self.fc1(x)
        x = self.drop(x)
        x = self.fc2(x)

        return x

model = CNNClassifier()

criterion = torch.nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

def train_model(model, optimizer, criterion, epochs=8):
    for child in model.children():
      if hasattr(child, 'reset_parameters'):
        child.reset_parameters()
    model.to(device)
    model.train()

    # TODO: Implement your training loop as specified in the description
    loss_list = []
    accuracy_list = []
    for epoch in range(epochs):
      running_loss = 0.0
      correct = 0
      for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        optimizer.zero_grad()
        loss = criterion(outputs, labels) # this is the average loss for one batch of inputs
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        prediction = torch.argmax(outputs,1)
        correct += (prediction == labels).float().sum().item()
      loss_list.append(running_loss / len(train_loader))
      accuracy_list.append(correct / len(train_dataset))
  
      # print statistics
      print(f'The loss for Epoch {epoch} is: {running_loss/len(train_loader)}')
      print(f'The accuracy for Epoch {epoch} is: {correct/len(train_dataset)}')


    final_training_accuracy = accuracy_list[-1]      # TODO: Set this to final training accuracy
    final_training_loss = loss_list[-1]         # TODO: Set this to final testing accuracy
    return final_training_loss, final_training_accuracy, loss_list, accuracy_list

loss_list = []
accuracy_list = []
training_loss, training_accuracy, loss_list, accuracy_list = train_model(model, optimizer, criterion)


print("The training loss is ", str(training_loss))
print("The training accuracy is ", str(training_accuracy))

def test_model(model, criterion):
    model.eval()

    # TODO: Implement your testing loop with the `test_loader` data
    test_loss = 0
    correct = 0
    with torch.no_grad():
      for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        test_loss += criterion(outputs, labels).item()
        predicted = torch.argmax(outputs, 1)
        correct += (predicted == labels).float().sum().item()

    testing_loss =  test_loss / len(test_loader)        # TODO: Set this to your testing loss
    testing_accuracy = correct / len(test_dataset)     # TODO: Set this to your testing accuracy
    return  testing_loss, testing_accuracy
testing_loss, testing_accuracy = test_model(model, criterion)

print("The testing loss is ", str(testing_loss))
print("The testing accuracy is ", str(testing_accuracy))